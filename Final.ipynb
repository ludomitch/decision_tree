{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT IMPORT PANDAS\n",
    "### Lib imports\n",
    "import tree as dt\n",
    "import evaluation as ev\n",
    "import numpy as np\n",
    "import copy\n",
    "import config as cfg\n",
    "import cross_val as cv\n",
    "import tree_plot as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('noisy_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with hyperparameters: {'depth': 2, 'boundary': 2, 'prune': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacha/Bureau/Imperial/Intro_to_ML/decision_tree/evaluation.py:62: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with hyperparameters: {'depth': 4, 'boundary': 2, 'prune': True}\n",
      "Running with hyperparameters: {'depth': 6, 'boundary': 2, 'prune': True}\n",
      "Running with hyperparameters: {'depth': 8, 'boundary': 2, 'prune': True}\n",
      "Running with hyperparameters: {'depth': 10, 'boundary': 2, 'prune': True}\n",
      "Running with hyperparameters: {'depth': 12, 'boundary': 2, 'prune': True}\n",
      "\n",
      "Best averaged confusion matrix : \n",
      " [[49.33333333  1.33333333  1.33333333  2.44444444]\n",
      " [ 2.         49.22222222  2.77777778  1.22222222]\n",
      " [ 2.22222222  2.22222222 50.66666667  2.11111111]\n",
      " [ 2.33333333  1.55555556  1.44444444 50.        ]]\n"
     ]
    }
   ],
   "source": [
    "best_hyper, F1_score, err_all_hyperparams, cm = cv.param_tuning(data, folds=10, test_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = cm.copy()\n",
    "recall_vect = np.zeros((1, cfg.NB_LABELS))\n",
    "prec_vect = np.zeros((1, cfg.NB_LABELS))\n",
    "classification_rate = np.zeros((1, cfg.NB_LABELS))\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "\n",
    "        recall_vect[0, i] = min(\n",
    "            confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]),\n",
    "            confusion_matrix[i, i],\n",
    "        )  # Recall = TP/(TP + FN)\n",
    "\n",
    "        prec_vect[0, i] = min(\n",
    "            confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]),\n",
    "            confusion_matrix[i, i],\n",
    "        )  # Precision = TP/(TP + FP)\n",
    "\n",
    "        classification_rate[0, i] = np.trace(confusion_matrix) / (\n",
    "            np.trace(confusion_matrix)\n",
    "            + np.sum(confusion_matrix[:, i])\n",
    "            + np.sum(confusion_matrix[i, :])\n",
    "            - 2 * confusion_matrix[i, i]\n",
    "        )  # Classification rate = (TP + TN)/(TP + TN + FP + FN)\n",
    "        \n",
    "uar = np.mean(recall_vect)  # Unweighted Average Recall\n",
    "uap = np.mean(prec_vect)  # Unweighted Average Precision\n",
    "f1 = 2 * (uar * uap) / (uar + uap)  # Compute F1\n",
    "uac = np.mean(classification_rate)  # Unweighted Averate Classification Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_vect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prec_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = []\n",
    "for i in range(recall_vect.shape[1]):\n",
    "    f2.append(2*(recall_vect[0,i] * prec_vect[0,i])/(recall_vect[0,i] + prec_vect[0,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = np.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = tp.final_plot(best_hyper, data= \"noisy_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "data = np.loadtxt('noisy_dataset.txt')\n",
    "for i in range(data.shape[0]):\n",
    "    if ev.predict(tree, data[i,:]) == data[i,-1]:\n",
    "        correct +=1\n",
    "print(correct/i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
